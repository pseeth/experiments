{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeparationModel(\n",
      "  (layers): ModuleDict(\n",
      "    (embedding): Embedding(\n",
      "      (linear): Linear(in_features=600, out_features=5120, bias=True)\n",
      "    )\n",
      "    (mel_projection): MelProjection()\n",
      "    (recurrent_stack): RecurrentStack(\n",
      "      (rnn): LSTM(256, 300, num_layers=4, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of parameters: 10910720\n"
     ]
    }
   ],
   "source": [
    "!cd .. \\\n",
    "    && python config/config.py --config_folder runs/run0/config dataset wsj \\\n",
    "    && python config/config.py --config_folder runs/run0/config dpcl_recurrent \\\n",
    "    && python config/config.py --config_folder runs/run0/config \\\n",
    "                                train --training_folder /pipeline/data/wsj0-mix/2speakers_anechoic/wav8k/min/tr/ \\\n",
    "                                     --validation_folder /pipeline/data/wsj0-mix/2speakers_anechoic/wav8k/min/cv/ \\\n",
    "                                     --output_folder runs/run0 \\\n",
    "                                     --loss_function dpcl embeddings .5 \\\n",
    "                                     --num_workers 10 \\\n",
    "    && python train.py --train runs/run0/config/train.json \\\n",
    "                             --model runs/run0/config/dpcl_recurrent.json \\\n",
    "                             --dataset runs/run0/config/dataset.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: config.py train [-h]\n",
      "                       [--training_folder [TRAINING_FOLDER [TRAINING_FOLDER ...]]]\n",
      "                       [--validation_folder VALIDATION_FOLDER]\n",
      "                       [--output_folder OUTPUT_FOLDER]\n",
      "                       [--num_epochs NUM_EPOCHS]\n",
      "                       [--learning_rate LEARNING_RATE]\n",
      "                       [--learning_rate_decay LEARNING_RATE_DECAY]\n",
      "                       [--patience PATIENCE] [--batch_size BATCH_SIZE]\n",
      "                       [--num_workers NUM_WORKERS]\n",
      "                       [--loss_function FUNCTION TARGET WEIGHT]\n",
      "                       [--optimizer {adam,rmsprop,sgd}] [--device {cuda,cpu}]\n",
      "                       [--data_parallel] [--curriculum_learning]\n",
      "                       [--sample_strategy {sequential,random}]\n",
      "                       [--initial_length INITIAL_LENGTH]\n",
      "                       [--weight_decay WEIGHT_DECAY]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --training_folder [TRAINING_FOLDER [TRAINING_FOLDER ...]]\n",
      "                        Path(s) to folder(s) contianing training data.\n",
      "                        (default: )\n",
      "  --validation_folder VALIDATION_FOLDER\n",
      "                        Path to folder containing validation data. (default: )\n",
      "  --output_folder OUTPUT_FOLDER\n",
      "                        Path to folder to write output to (this includes logs\n",
      "                        & checkpoints). (default: )\n",
      "  --num_epochs NUM_EPOCHS\n",
      "                        Number of training epochs. One epoch means one run\n",
      "                        through all given training data. (default: 100)\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        Weighting of backprop delta. (default: 0.001)\n",
      "  --learning_rate_decay LEARNING_RATE_DECAY\n",
      "                        Rate at which to decay learning rate. A learning rate\n",
      "                        of 0.5 means the learning rate is halved every\n",
      "                        <patience=5> epochs that the change in the loss\n",
      "                        function is below some epsilon (default: 0.5)\n",
      "  --patience PATIENCE   Number of epochs of minimal (within some epsilon)\n",
      "                        change in loss function required before decaying\n",
      "                        learning rate (default: 5)\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Number of training samples per batch. (default: 40)\n",
      "  --num_workers NUM_WORKERS\n",
      "                        ? - clarify wording (default: 10)\n",
      "  --loss_function FUNCTION TARGET WEIGHT\n",
      "                        Triple of loss function, target model output on which\n",
      "                        to compute loss function and weight. `FUNCTION` may be\n",
      "                        any of the following: ['L1', 'DPCL', 'MSE', 'KL'].\n",
      "                        Multiple loss function triples may be specified, each\n",
      "                        triple to its own `-loss_function` flag. E.g. to\n",
      "                        specify two different loss functions: `...\n",
      "                        --loss_function mse masks .4 --loss_function dpcl\n",
      "                        embeddings .6` (default: [])\n",
      "  --optimizer {adam,rmsprop,sgd}\n",
      "                        Optimizer for gradient descent (default: adam)\n",
      "  --device {cuda,cpu}   Train using GPU or CPU. (default: cuda)\n",
      "  --data_parallel       Train across all available GPUs using DataParallel.\n",
      "                        (default: True)\n",
      "  --curriculum_learning\n",
      "                        Whether or not to perform curriculum learning\n",
      "                        (default: True)\n",
      "  --sample_strategy {sequential,random}\n",
      "                        Strategy for sampling training examples. (default:\n",
      "                        sequential)\n",
      "  --initial_length INITIAL_LENGTH\n",
      "                        Fraction of initial length to use (for curriculum\n",
      "                        learning) (default: 400)\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "                        For L2 regularization (default: 0.0)\n",
      "usage: train.py [-h] --train TRAIN --model MODEL --dataset DATASET\n",
      "\n",
      "Parse {model, dataset, train} JSONs\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help         show this help message and exit\n",
      "  --train TRAIN      Path to JSON containing training configuration\n",
      "  --model MODEL      Path to JSON containing model configuration\n",
      "  --dataset DATASET  Path to JSON containing dataset configuration\n"
     ]
    }
   ],
   "source": [
    "!cd .. \\\n",
    "    && python config/config.py train -h\n",
    "\n",
    "!cd .. \\\n",
    "    && python train.py -h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
